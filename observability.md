# Observability: Telemetry and Logs

## Overview

In this section, we will learn about a couple of monitoring (Prometheus), tracing (Zipkin), and data visualization tools (Grafana).

For Grafana and Kiali to work, we will first have to install the Prometheus addon.

## Observability and Prometheus 

Thanks to the sidecar deployment model where Envoy proxies run next to application instances and intercpt traffic, these proxies collect metrics.

The metrics Envoy proxies collect and helping us get visibility into the state of your system. Gaining this visibility into our systems is critical because we need to understand what's happening and empower the operators to troubleshoot, maintain, and optimize applications. 

Istio generates three types of telemetry to provide observability to services in the mesh:
- Metrics
- Distributed Traces
- Access Logs

### Metrics

Istio generates metrics based on the four golden signals: latency, traffic, errors and saturation.

<strong>Latency</strong> represents the time it takes to service a request. These metrics should be broken down into latency of succesful requests (e.g. HTTP 200) and failed requests (e.g. HTTP 500).

<strong>Traffic</strong> measures how much demand gets placed on the system, and it's measured in system-specific metrics. For example, HTTP requests per second, or concurrent sessions, retrievals per second, and so on.

<strong>Errors</strong> measures the rate of failed requests (e.g. HTTP 500s)

<strong>Saturation</strong>measures how full the most constrained resources of services are. For example, utilization of a thread pool.

The metrics are collected at different levels, starting with the most granular, the Envoy proxy-level, then the service-level and control plane metrics.

#### Proxy-level Metrics

Envoy has a crucial role in generating metrics. It generates a rich set of metrics about all traffic passing through it. Using the metrics generated by Envoy, we can monitor the mesh at the lowest granularity, for example, metrics for individual listeners and clusters in the Envoy proxy.

We can control which Envoy metrics get generated and collected at each workload instance as a mesh operator.

#### Service-level Metrics

The service level metrics cover the four golden signals we mentioned earlier. These metrics allow us to monitor service-to-service communication. Additionally, Istio comes with dashboards to monitor the service behavior based on these metrics.

Just like with the proxy-level metrics, the operator can customize which service-level metrics Istio collects.

Istio exports the standard set of metrics to Prometheus by default.

 #### Control-plane Metrics

 Istio also emits control plane metrics that can help monitor the control plane and behavior of Istio, not user services.

You can find the full list of exported control plane metrics [here](https://istio.io/latest/docs/reference/commands/pilot-discovery/#metrics).

The control plane metrics include the number of conflicting inbound/outbound listeners, the number of clusters without instances, rejected or ignored configurations, and so on.

## Prometheus

Prometheus is an open-source monitoring system and time series database. Istio uses Prometheus to record metrics that track the health of Istio and applications in the mesh.

To install Prometheus, we can use the sample installation file;

```
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus created
deployment.apps/prometheus created
To open the Prometheus dashboard, we can use the dashboard command in the Istio CLI:

$ getmesh istioctl dashboard prometheus
http://localhost:9090
```

### Deploying a Sample App

To see some requests and traffic we will deploy an Nginx instance:

```
$ kubectl create deploy my-nginx --image=nginx
deployment.apps/my-nginx created
```

To generate some traffic and access the Nginx Pod, we need to make it accessible somehow.

The simplest way is to expose the Nginx deployment as a Kubernetes LoadBalancer service:

```
kubectl expose deployment my-nginx --type=LoadBalancer --name=my-nginx --port 80
```

Note: later in the course, we will learn how to use Istio resources and expose the services through Istios’ ingress gateway.

Now we can run kubectl get services and get the external IP address of the my-nginx services:

```
$ kubectl get svc
NAME         TYPE           CLUSTER-IP   EXTERNAL-IP      PORT(S)        AGE
kubernetes   ClusterIP      10.48.0.1    <none>           443/TCP        73m
my-nginx     LoadBalancer   10.48.0.94   [IP HERE]   80:31191/TCP   4m6s
```

Let’s store that IP address as an environment variable so we can use it throughout this lab:

```
export NGINX_IP=$(kubectl get service my-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
```

You can now run curl against the above IP and you should get back the default Nginx page:

```
$ curl $NGINX_IP
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

Let’s make a couple of requests to the $NGINX_IP environment variable we’ve created initially. Then, from the Prometheus UI, you can search for one of the Istio metrics (istio_requests_total, for example) to understand which data points are being collected.

## Grafana

Grafana is an open platform for analytics and monitoring. Grafana can connect to various data sources and visualizes the data using graphs, tables, heatmaps, etc. With a powerful query language, you can customize the existing dashboard and create more advanced visualizations.

With Grafana, we can monitor the health of Istio installation and applications running in the service mesh.

We can use the grafana.yaml to deploy a sample installation of Grafana with pre-configured dashboards.

Ensure you deploy the Prometheus add-on before deploying Grafana, as Grafana uses Prometheus as its data source.

Run the following command to deploy Grafana with pre-configured dashboards:

```
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml
serviceaccount/grafana created
configmap/grafana created
service/grafana created
deployment.apps/grafana created
configmap/istio-grafana-dashboards created
configmap/istio-services-grafana-dashboards created
```

Kubernetes deploys Grafana in the istio-system namespace. To access Grafana, we can use the getmesh istioctl dashboard command:

```
$ getmesh istioctl dashboard grafana
http://localhost:3000
```

We can open http://localhost:3000 in the browser to go to Grafana. Then, click Home and the istio folder to see the installed dashboards, as shown in the figure below.

The Istio Grafana installation comes pre-configured with the following dashboards:

1. Istio Control Plane Dashboard

From the Istio control plane dashboard, we can monitor the health and performance of the Istio control plane.

This dashboard will show us the control plane’s resource usage (memory, CPU, disk, Go routines) and information about the pilot, Envoy, and webhooks.

2. Istio Mesh Dashboard

The mesh dashboard provides us an overview of all services running in the mesh. The dashboard includes the global request volume, success rate, and the number of 4xx and 5xx responses.

3. Istio Performance Dashboard

The performance dashboard shows us the Istio main components cost in resource utilization under a steady load.

4. Istio Service Dashboard

The service dashboard allows us to view details about our services in the mesh.

We can get information about the request volume, success rate, durations, and detailed graphs showing incoming requests by source and response code, duration, and size.

5. Istio Wasm Extension Dashboard

The Istio Wasm extension dashboards show the metrics related to WebAssembly modules. From this dashboard, we can monitor the active and created Wasm VMs, data about fetching remote Wasm modules, and proxy resource usage.

6. Istio Workload Dashboard

This dashboard provides us a detailed breakdown of metrics for a workload.

## Distributed Tracing

Distributed tracing is a method for monitoring microservice applications. Using distributed tracing, we can follow the requests as they travel through the different pieces of the system being monitored.

Envoy generates a unique request ID and tracing information and stores it as part of HTTP headers whenever a request enters the service mesh. Any application can then forward these headers to other services to create a full trace through the system.

A distributed trace is a collection of spans. As requests flow through different system components, each component contributes a span. Each span has a name, start and finish timestamp, a set of key-value pairs called tags and logs, and a span context.

Tags get applied to the whole span, and we can use them for querying and filtering. Here’s a couple of examples of tags we’ll see when using Zipkin. Note that some of these are generic, and some of them are Istio specific:

- istio.mesh_id
- istio.canonical_service
- upstream_cluster
- http.url
- http.status_code
- zone

The individual spans and the context headers that identify the span, parent span, and trace ID get sent to the collector component. The collector validates, indexes and stores the data.

The Envoy proxies automatically send the individual spans as the requests flow through them. Note that Envoy can only collect spans at the edges. We’re responsible for generating any other spans within each application and ensuring that we forward the tracing headers whenever we make calls to other services. This way, the individual spans can be correlated correctly into a single trace.

### Distributed Tracing with Zipkin

Zipkin is a distributed tracing system. We can easily monitor distributed transactions in the service mesh and discover any performance or latency issues.

For our services to participate in a distributed trace, we need to propagate HTTP headers from the services when making any downstream service calls. Even though all requests go through an Istio sidecar, Istio has no way of correlating the outbound requests to the inbound requests that caused them. By propagating the relevant headers from your applications, you can help Zipkin stitch together the traces.

Istio relies on B3 trace headers (headers starting with x-b3) and the Envoy-generated request ID (x-request-id). The B3 headers are used for trace context propagation across service boundaries.

Here are the specific header names we need to propagate in our applications with each outgoing request:

- x-request-id
- x-b3-traceid
- x-b3-spanid
- x-b3-parentspanid
- x-b3-sampled
- x-b3-flags
- b3

The most common way to propagate the headers is to copy them from the incoming request and include them in all outgoing requests made from your applications.

Traces you get with Istio service mesh get captured at the service boundaries. To understand the application behavior and troubleshoot problems, you need to instrument your applications by creating additional spans properly.

To install Zipkin, we can use the zipkin.yaml file:

```
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml
deployment.apps/zipkin created
service/tracing created
service/zipkin created
```

We can open the Zipkin dashboard by running getmesh istioctl dashboard zipkin. From the UI we can select the criteria for the trace lookups. Click the button and select serviceName and then my-nginx.default service from the dropdown and click the search button (or press Enter) to search the traces.

We can click on individual traces to dig deeper into the different spans. The detailed view will show us the duration of calls between the services and the request details, such as method, protocol, status code, and similar. Since we only have one service running (Nginx), you won’t see many details. Later on, we will return to Zipkin and explore the traces in more detail.

### Mesh Observability with Kiali

Kiali is a management console for Istio-based service mesh. It provides dashboards, observability and lets us operate the mesh with robust configuration and validation capabilities. It shows the service mesh structure by inferring traffic topology and displays the health of the mesh. Kiali provides detailed metrics, robust validation, Grafana access, and strong integration for distributed tracing with Jaeger.

To install Kiali, use the kiali.yaml file:

```
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml
customresourcedefinition.apiextensions.k8s.io/monitoringdashboards.monito
ring.kiali.io created
serviceaccount/kiali created
configmap/kiali created
clusterrole.rbac.authorization.k8s.io/kiali-viewer created
clusterrole.rbac.authorization.k8s.io/kiali created
clusterrolebinding.rbac.authorization.k8s.io/kiali created
service/kiali created
deployment.apps/kiali created
```

Note that if you see any errors such as no matches for kind "MonitoringDashboard" in version "monitoringkiali.io/v1alpha", re-run the kubectl apply command again. The issue is that there might be a race condition when installing the CRD (custom resource definition) and resources that that CRD defines.

We can open Kiali using getmesh istioctl dashboard kiali.

The graph shows us the service topology and visualizes how the services communicate. It also shows the inbound and outbound metrics and traces by connecting to Jaeger and Grafana (if installed). Colors in the graph represent the health of the service mesh. A node colored red or orange might need attention. The color of an edge between components represents the health of the requests between those components. The node shape indicates the type of components, such as services, workloads, or apps.

The health of nodes and edges gets refreshed automatically based on the user’s preference. We can also pause the graph to examine a particular state or replay it to re-examine a specific period.

Kiali provides actions to create, update, and delete Istio configuration driven by wizards. We can configure request routing, fault injection, traffic shifting, and request timeouts, all from the UI. If any existing Istio configuration is already deployed, Kiali can validate it and report any warnings or errors.